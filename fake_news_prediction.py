# -*- coding: utf-8 -*-
"""Fake News Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nhuF17ykXbvSROiDTECpVlRd7XLGHSRo

**About the dataset**

1. id : unique id for a news article
2. title : the title of a news article
3. author : author of news article
4. text : the text of the article(could be incomplete)
5. label : a label that marks whether the news article is real or fake


real news--> 0

fake news --> 1

---

Importing Dependencies
"""

import numpy as np
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

import nltk
nltk.download('stopwords')

# printing the stopwords in English

print(stopwords.words('english'))

"""Data Collection & Data Pre-processing"""

# loading the datasets to a panda DataFrame

news_training_dataset = pd.read_csv('/content/train.csv')
news_test_dataset = pd.read_csv('/content/test.csv')

news_training_dataset.shape

news_test_dataset.shape

news_training_dataset.head()

news_training_dataset.tail()

news_test_dataset.head()

news_test_dataset.tail()

# counting the number of missing values in the training dataset

news_training_dataset.isnull().sum()

# counting the number of missing values in the test dataset

news_test_dataset.isnull().sum()

# replacing the null value with empty string

news_training_dataset = news_training_dataset.fillna('')
news_test_dataset = news_test_dataset.fillna('')

news_training_dataset.isnull().sum()

news_test_dataset.isnull().sum()

# merging the author name & news title

news_training_dataset['content'] = news_training_dataset['author'] + ' ' + news_training_dataset['title']
news_test_dataset['content'] = news_test_dataset['author'] + ' ' + news_test_dataset['title']

print(news_training_dataset['content'])

print(news_test_dataset['content'])

# separating the data  & label for training dataset

X_train = news_training_dataset.drop('label' , axis=1)
Y_train = news_training_dataset['label']

print(X_train)

print(Y_train)

print(news_test_dataset)

print(news_test_dataset.columns)

X_test = news_test_dataset

"""**Stemming Procedure :**

Stemming is the process of reducing a Word to its Root Word.

example:
actor, actress, acting ---> act
"""

port_stem = PorterStemmer()

def stemming(content):
    #removing all the other letters,numbers or symbols or anything else and replace them with space(' ') except (a-z) & (A-Z)
    stemmed_content = re.sub('[^a-zA-Z]',' ',content)

     # convert all to lower case
    stemmed_content = stemmed_content.lower()

    # all will be split and turned into a list
    stemmed_content = stemmed_content.split()

    # stemming process except all the stopwords
    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
    stemmed_content = ' '.join(stemmed_content)
    return stemmed_content

news_training_dataset['content'] = news_training_dataset['content'].apply(stemming)

news_test_dataset['content'] = news_test_dataset['content'].apply(stemming)

print(news_training_dataset['content'])

print(news_test_dataset['content'])

X_train = news_training_dataset['content'].values
Y_train = news_training_dataset['label'].values

X_test = news_test_dataset['content'].values

print(X_train)

print(Y_train)

Y_train.shape

# converting textual data into numerical data for training data

vectorizer = TfidfVectorizer()
vectorizer.fit(X_train)

X_train = vectorizer.transform(X_train)

# converting textual data into numerical data for test data

#vectorizer = TfidfVectorizer()
#vectorizer.fit(X_test)

X_test= vectorizer.transform(X_test)

print(X_train)

print(X_test)

"""Splitting the dataset into **Training Data** & **Test Data**"""

# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=2)

"""Training the Model : **Logistic Regression**"""

model = LogisticRegression()

model.fit(X_train, Y_train)

"""Evaluation: Accuracy Score

"""

# accuracy score on the training data

X_train_prediction = model.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)

print('Accuracy Score of the Training Data : ' , training_data_accuracy)

# accuracy score on the test data

# X_test_prediction = model.predict(X_test)
# test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

#print('Accuracy Score of the Test Data : ' , test_data_accuracy)

"""Making a Predictive System"""

X_new = X_test[11]
prediction = model.predict(X_new)
print(prediction)

if (prediction[0]==0):
  print('The news is Real')
else:
  print('The news is Fake')

import numpy as np

# Assuming X_new contains multiple samples
predictions = model.predict(X_test)

# Count the number of occurrences of each class
unique, counts = np.unique(predictions, return_counts=True)
prediction_counts = dict(zip(unique, counts))

# Plotting the distribution
plt.figure(figsize=(9, 8))
plt.bar(prediction_counts.keys(), prediction_counts.values(), color=['blue', 'red'])
plt.title('Distribution of Predicted Labels')
plt.xlabel('Class Label')
plt.ylabel('Count')
plt.xticks([0, 1], labels=['Real News', 'Fake News'])  # Assuming binary classification
plt.show()

import matplotlib.pyplot as plt

# Predicting probabilities for the positive class (e.g., "Fake News")
prediction_probabilities = model.predict_proba(X_test)[:, 1]

# Define the decision threshold
threshold = 0.5

# Create a scatter plot with colors based on the threshold
plt.figure(figsize=(10, 6))
plt.scatter(range(len(prediction_probabilities)), prediction_probabilities,
            c=(prediction_probabilities >= threshold), cmap='bwr', edgecolor='k', s=100)
plt.axhline(y=threshold, color='red', linestyle='--', label=f'Threshold = {threshold}')
plt.title('Prediction Probabilities with Decision Threshold')
plt.xlabel('Sample Index')
plt.ylabel('Probability')
plt.ylim(0, 1)
plt.colorbar(label='Prediction >= Threshold (1 = Yes, 0 = No)')
plt.grid(True)
plt.legend()
plt.show()

